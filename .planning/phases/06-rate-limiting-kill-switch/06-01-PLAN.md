---
phase: 06-rate-limiting-kill-switch
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ratelimit.rs
  - src/lib.rs
  - src/protocol/jsonrpc.rs
autonomous: true
requirements: [RATE-01, RATE-02, RATE-03]

must_haves:
  truths:
    - "RateLimiter enforces per-client per-tool limits using token bucket"
    - "Rate limit config uses per-tool overrides with a default RPM fallback"
    - "Exhausted bucket returns retry-after seconds (time until next refill)"
  artifacts:
    - path: "src/ratelimit.rs"
      provides: "TokenBucket struct and RateLimiter with Mutex<HashMap>"
      exports: ["RateLimiter"]
      min_lines: 60
    - path: "src/protocol/jsonrpc.rs"
      provides: "RATE_LIMIT_ERROR and KILL_SWITCH_ERROR constants"
      contains: "RATE_LIMIT_ERROR"
  key_links:
    - from: "src/ratelimit.rs"
      to: "src/config/types.rs"
      via: "RateLimiter::new takes &RateLimitConfig"
      pattern: "RateLimitConfig"
---

<objective>
Implement the in-memory token bucket rate limiter module and add new JSON-RPC error code constants.

Purpose: Provides the rate limiting data structure that the dispatch loop will call in plan 02 to throttle per-client per-tool traffic.
Output: `src/ratelimit.rs` with `RateLimiter` struct and unit tests, updated error constants in `jsonrpc.rs`.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-rate-limiting-kill-switch/06-RESEARCH.md
@src/config/types.rs
@src/protocol/jsonrpc.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add error code constants and error_with_data helper</name>
  <files>src/protocol/jsonrpc.rs</files>
  <action>
Add two new error code constants after the existing INTERNAL_ERROR constant:

```rust
pub const RATE_LIMIT_ERROR: i32 = -32004;
pub const KILL_SWITCH_ERROR: i32 = -32005;
```

Add a constructor to `JsonRpcResponse` for errors with a `data` field (needed for retry-after semantics):

```rust
pub fn error_with_data(id: JsonRpcId, code: i32, message: String, data: Value) -> Self {
    Self {
        jsonrpc: "2.0".to_string(),
        id,
        result: None,
        error: Some(JsonRpcError {
            code,
            message,
            data: Some(data),
        }),
    }
}
```

This keeps the existing `error()` constructor unchanged (no data field) while providing the new variant for rate limit responses that need `retryAfter`.
  </action>
  <verify>Run `cargo build --release 2>&1 | tail -5` -- no errors or warnings.</verify>
  <done>RATE_LIMIT_ERROR (-32004) and KILL_SWITCH_ERROR (-32005) constants exist. `error_with_data` constructor exists on JsonRpcResponse.</done>
</task>

<task type="auto">
  <name>Task 2: Implement RateLimiter module with unit tests</name>
  <files>src/ratelimit.rs, src/lib.rs</files>
  <action>
Create `src/ratelimit.rs` with the following implementation (no new dependencies -- stdlib only):

1. **TokenBucket struct** (private):
   - Fields: `tokens: u32`, `max_tokens: u32`, `last_refill: Instant`
   - `new(max_tokens: u32)` -- starts full
   - `try_consume(&mut self) -> Result<(), f64>` -- lazy refill on access (if >=60s elapsed, reset tokens to max and update last_refill). If tokens > 0, decrement and return Ok. Otherwise return Err(seconds_until_next_refill) where retry = max(1.0, 60.0 - elapsed).

2. **RateLimiter struct** (public):
   - Fields: `buckets: Mutex<HashMap<(String, String), TokenBucket>>`, `default_rpm: u32`, `per_tool: HashMap<String, u32>`
   - `pub fn new(config: &RateLimitConfig) -> Self` -- clones per_tool from config
   - `pub fn check(&self, client: &str, tool: &str) -> Result<(), f64>` -- looks up RPM for tool (per_tool override or default_rpm), gets or creates bucket for (client, tool) key, calls try_consume

3. **Unit tests** in `#[cfg(test)] mod tests` at the bottom of the file:
   - `rate_limiter_allows_within_limit`: 5 RPM default, 5 calls succeed, 6th fails, different client still succeeds
   - `rate_limiter_per_tool_override`: per_tool override of 2 for "expensive_tool", 2 calls succeed, 3rd fails, default-limit tool still works
   - `rate_limiter_returns_positive_retry_after`: when exhausted, Err value is > 0.0 and <= 60.0
   - `rate_limiter_independent_per_client`: client1 exhausts limit, client2 for same tool is unaffected

Add `pub mod ratelimit;` to `src/lib.rs` (after the `pub mod protocol;` line).

Key design decisions per research:
- Use `std::sync::Mutex<HashMap>` (NOT DashMap) -- single stdio transport means zero contention
- Lazy refill on access (NOT background timer) -- zero resources when idle
- Accept stale bucket accumulation for v1 (~24 bytes per unique client+tool pair)
  </action>
  <verify>Run `cargo test ratelimit 2>&1 | tail -20` -- all 4 unit tests pass. Run `cargo build --release 2>&1 | tail -5` -- no warnings.</verify>
  <done>RateLimiter struct exists, constructs from RateLimitConfig, enforces per-client per-tool limits with retry-after semantics. All unit tests pass.</done>
</task>

</tasks>

<verification>
1. `cargo build --release` succeeds with no warnings
2. `cargo test ratelimit` -- all unit tests pass
3. `src/ratelimit.rs` exports `RateLimiter`
4. `src/protocol/jsonrpc.rs` has RATE_LIMIT_ERROR (-32004) and KILL_SWITCH_ERROR (-32005)
</verification>

<success_criteria>
- RateLimiter is a standalone module with no external dependencies beyond stdlib
- Unit tests prove per-client per-tool isolation, per-tool overrides, and retry-after semantics
- Error constants are ready for use by the dispatch loop in plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/06-rate-limiting-kill-switch/06-01-SUMMARY.md`
</output>
